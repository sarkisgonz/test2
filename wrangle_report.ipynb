{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6497f7",
   "metadata": {},
   "source": [
    "<h1> <center> Wrangle Report <center\\> <h1\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6eb7b1",
   "metadata": {},
   "source": [
    "This report aims to describe the gathering, assessing and cleaning process used to perform the analysis in to the WeRateDogs tweets dataset.\n",
    "\n",
    "## Data Gather\n",
    "\n",
    "The first part of the process is the data gather. I first readed the 'twitter-archive-enhanced.csv' file that was given on hand using the pandas function pd.read_csv.\n",
    "\n",
    "Then I programmaticaly downloaded the file 'image-predictions.tsv' from udacity servers using the requests library, the contents were saved using the open() function.\n",
    "\n",
    "Finally, using the access library 'tweepy'. I accessed the Twitter's API and created a loop that extracts the favorite count and retweet count from all the tweets in the 'twitter-archive-enhanced.csv' and saves the content line by line in a txt file. Afterwards, the file was readed using pandas read_csv() function.\n",
    "\n",
    "\n",
    "## Data Assess\n",
    "\n",
    "The second part of the process is the data assess. Since there is a lot of issues in the data, I first started by deciding what are the questions I want to answer in my analysis, in order to clean only the neccessary issues.\n",
    "\n",
    "Then I started with a visual assessment by displaying the entire dataframes and scrolling to find issues like for example, there are a lot of missing values in the dataset.\n",
    "\n",
    "Then I used programatic assessment using functions like .info(), .sample(), .value_counts() and .duplicated(), at the end of the second part, I provided a list of issues the dataset has, to be cleaned in the third part.\n",
    "\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "The third part of the process is the data cleaning. I iterated a couple times when I was performing the analysis. Since the data is very untidy and messy, I only cleaned the issues that were more important for the analysis I have selected. \n",
    "\n",
    "Twelve issues were solved in the in the data cleaning, two tidiness issues and 10 quality issues. The main issues were that there are many retweets and replies that do not correspond to original data and that for some tweets, non of the predictions are dog breeds, which is not usefull for my analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
